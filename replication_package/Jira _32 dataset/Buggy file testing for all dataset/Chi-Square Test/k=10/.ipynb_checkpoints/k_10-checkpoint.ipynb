{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f0d061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining activemq-client/src/main/java/org/apache/activemq/transport/reliable/ReplayBufferListener.java with LIME\n",
      "{'Added_lines': 1, 'Del_lines': -1, 'CountLine': -1, 'CountPath_Max': -1, 'CountStmtExe': 1, 'CountLineCodeExe': -1, 'CountPath_Mean': -1, 'CountSemicolon': -1, 'CountStmt': 1, 'CountLineCode': 1}\n",
      "{'Added_lines': 1, 'Del_lines': 1, 'CountStmt': 1, 'CountSemicolon': 1, 'CountLine': 1, 'CountLineCodeExe': -1, 'CountStmtExe': -1, 'CountLineCode': 1, 'CountPath_Max': -1, 'CountPath_Mean': 1}\n",
      "{'Added_lines': 1, 'CountSemicolon': 1, 'CountLine': 1, 'Del_lines': 1, 'CountLineCode': 1, 'CountPath_Max': 1, 'CountPath_Mean': 1, 'CountStmt': 1, 'CountLineCodeExe': 1, 'CountStmtExe': 1}\n",
      "set top_k_rules failed, top_k_rules should be int in range 1 - 15 (both included)\n",
      "PyExplainer can not find rules to avoid!\n",
      "This could lead to blank explanation UI!\n",
      "Please check whether the global model is properly trained with sufficient training data.\n",
      "actual value of CountLineCodeExe < 0, currently do not support this type of rule\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d24265e86649d895b8abdc3adb6069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Risk Score: '), FloatProgress(value=0.0, bar_style='info', layout=Layout(width='40…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b38676ec454b4eb700e7843760ad04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='3px solid black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d0e3ca81e54e549ad085a42553040d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=31.0, continuous_update=False, description='#1 The value of CountLine is less than 31', layo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf753af62bf459f92e87a3361db5e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=2.0, continuous_update=False, description='#2 The value of CountSemicolon is more than 2', l…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964ed92a8ec14861b3578f2a2aae567c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.0, continuous_update=False, description='#3 The value of CountLineCodeExe is less than 0',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set top_k_rules failed, top_k_rules should be int in range 1 - 15 (both included)\n",
      "PyExplainer can not find rules to avoid!\n",
      "This could lead to blank explanation UI!\n",
      "Please check whether the global model is properly trained with sufficient training data.\n",
      "set top_k_rules failed, top_k_rules should be int in range 1 - 15 (both included)\n",
      "PyExplainer can not find rules to avoid!\n",
      "This could lead to blank explanation UI!\n",
      "Please check whether the global model is properly trained with sufficient training data.\n",
      "actual value of CountLineCodeExe < 0, currently do not support this type of rule\n",
      "{'CountLine': -1, 'CountSemicolon': 1, 'CountLineCodeExe': 1}\n"
     ]
    }
   ],
   "source": [
    "## Load Data and preparing datasets\n",
    "\n",
    "# Import for Load Data\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "\n",
    "# Import for Split Data into Training and Testing Samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import subprocess\n",
    "import sys\n",
    "import importlib\n",
    "import numpy\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from pyBreakDown.explainer import Explainer\n",
    "from pyBreakDown.explanation import Explanation\n",
    "from pyexplainer.pyexplainer_pyexplainer import PyExplainer\n",
    "import io\n",
    "import contextlib\n",
    "import re\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# feature agreement\n",
    "def calculate_task1(dict1, dict2):\n",
    "    if len(dict1) == 0 or len(dict2) == 0:\n",
    "        return 0.00\n",
    "    keys1 = set(dict1.keys())\n",
    "    keys2 = set(dict2.keys())\n",
    "    common_keys = len(keys1.intersection(keys2))\n",
    "    return common_keys / max(len(dict1), len(dict2))\n",
    "\n",
    "# Rank Agreement\n",
    "def calculate_task2(dict1, dict2):\n",
    "    if len(dict1) == 0 or len(dict2) == 0:\n",
    "        return 0.00\n",
    "    keys1 = list(dict1.keys())\n",
    "    keys2 = list(dict2.keys())\n",
    "    common_keys = 0\n",
    "    for i in range(min(len(keys1), len(keys2))):\n",
    "        if keys1[i] == keys2[i]:\n",
    "            common_keys += 1\n",
    "    return common_keys / max(len(dict1), len(dict2))\n",
    "\n",
    "# Sign Agreement\n",
    "def calculate_task3(dict1, dict2):\n",
    "    if len(dict1) == 0 or len(dict2) == 0:\n",
    "        return 0.00\n",
    "    common_signs = 0\n",
    "    total_common = 0\n",
    "    for key in dict1:\n",
    "        if key in dict2:\n",
    "            if dict1[key] * dict2[key] > 0:\n",
    "                common_signs += 1\n",
    "            total_common += 1\n",
    "    if total_common == 0:\n",
    "        return 0.00\n",
    "    return common_signs / total_common\n",
    "\n",
    "file_list = glob(\"/Users/saumenduroy/Desktop/TOSEM_replicationpackage/data_jira/*.csv\")\n",
    "tested_file = []\n",
    "\n",
    "for file in file_list:\n",
    "    main_file=file\n",
    "    train_dataset = pd.read_csv(file, index_col = 'File')\n",
    "    \n",
    "    test_data_list = [f for f in file_list if f != file]\n",
    "    test_dataset = pd.DataFrame(columns=[\"File\"]+train_dataset.columns.values.tolist())\n",
    "    for test_file in test_data_list:\n",
    "        tmp_df = pd.read_csv(test_file)\n",
    "        test_dataset = pd.DataFrame(np.concatenate([test_dataset.values, tmp_df.values]), columns=tmp_df.columns)\n",
    "    test_dataset.set_index(\"File\", inplace=True)\n",
    "    outcome = 'RealBug'\n",
    "    features = ['CountPath_Max', 'CountPath_Mean', 'Del_lines', 'Added_lines', 'CountLine', 'CountStmt', 'CountLineCode', 'CountLineCodeExe', 'CountSemicolon', 'CountStmtExe']\n",
    "    # commits - # of commits that modify the file of interest\n",
    "    # Added lines - # of added lines of code\n",
    "    # Count class coupled - # of classes that interact or couple with the class of interest\n",
    "    # LOC - # of lines of code\n",
    "    # RatioCommentToCode - The ratio of lines of comments to lines of code\n",
    "\n",
    "    # process outcome to 0 and 1\n",
    "    train_dataset[outcome] = pd.Categorical(train_dataset[outcome])\n",
    "    train_dataset[outcome] = train_dataset[outcome].cat.codes\n",
    "\n",
    "    test_dataset[outcome] = pd.Categorical(test_dataset[outcome])\n",
    "    test_dataset[outcome] = test_dataset[outcome].cat.codes\n",
    "\n",
    "    X_train = train_dataset.loc[:, features]\n",
    "    X_test = test_dataset.loc[:, features]\n",
    "\n",
    "    y_train = train_dataset.loc[:, outcome]\n",
    "    y_test = test_dataset.loc[:, outcome]\n",
    "\n",
    "    class_labels = ['Clean', 'Defective']\n",
    "\n",
    "    X_train.columns = features\n",
    "    X_test.columns = features\n",
    "    training_data = pd.concat([X_train, y_train], axis=1)\n",
    "    testing_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    our_rf_model = RandomForestClassifier(random_state=0)\n",
    "    our_rf_model.fit(X_train, y_train)  \n",
    "    \n",
    "    #### LIME\n",
    "\n",
    "    # Import for LIME\n",
    "    \n",
    "    \n",
    "    file_to_be_explained = \"\"\n",
    "    \n",
    "    worked = True\n",
    "    \n",
    "    while worked:\n",
    "        try:\n",
    "            \n",
    "            random_file = random.choice(testing_data.index.values.tolist())\n",
    "            if random_file in tested_file:\n",
    "                continue\n",
    "            file_to_be_explained = random_file\n",
    "            # LIME Step 1 - Construct an explainer\n",
    "            our_lime_explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "                                        training_data = X_train.values,  \n",
    "                                        mode = 'classification',\n",
    "                                        training_labels = y_train,\n",
    "                                        feature_names = features,\n",
    "                                        class_names = class_labels,\n",
    "                                        discretize_continuous = True)\n",
    "\n",
    "            # LIME Step 2 - Use the constructed explainer with the predict function \n",
    "            # of your predictive model to explain any instance\n",
    "            lime_local_explanation_of_an_instance = our_lime_explainer.explain_instance(\n",
    "                                       # X_test[0],\n",
    "                                      # data_row = X_test.loc['FileName.py', : ], \n",
    "                                        data_row = X_test.loc[file_to_be_explained, : ],\n",
    "                                        predict_fn = our_rf_model.predict_proba, \n",
    "                                        num_features = len(features),\n",
    "                                        top_labels = 1)\n",
    "\n",
    "            #explainer = lime_tabular.LimeTabularExplainer(X_train, mode = \"regression\", feature_names = boston_housing.feature_names)\n",
    "            #explanation = explainer.explain_instance(X_test[0], model.predict, num_features = len(boston_housing.feature_names))\n",
    "            lime_prob = lime_local_explanation_of_an_instance.predict_proba\n",
    "            if lime_prob[1] >= 0.67:\n",
    "                print(f'Explaining {file_to_be_explained} with LIME')\n",
    "                lime_results = lime_local_explanation_of_an_instance.as_list()\n",
    "                lime_dict = {}\n",
    "                for results in lime_results:\n",
    "                    feat = \"\"\n",
    "                    for r in results:\n",
    "                        ignoring_sym = [\">=\", \">\", \"<=\", \"<\", \"=\"]\n",
    "                        if isinstance(r, str):\n",
    "                            feat = re.sub(r'[^a-zA-Z\\s_]', '', r).rstrip().lstrip()\n",
    "                        else:\n",
    "                            if r >= 0:\n",
    "                                lime_dict[feat] = 1\n",
    "                            else:\n",
    "                                lime_dict[feat] = -1\n",
    "                print(lime_dict)\n",
    "            \n",
    "                # Please use the code below to visualise the generated LIME explanation.\n",
    "                ### save lime output\n",
    "                # lime_local_explanation_of_an_instance.show_in_notebook()\n",
    "                \n",
    "                \n",
    "                ### SHAP:\n",
    "                file_to_be_explained_idx = list(X_test.index).index(file_to_be_explained)\n",
    "                explainer = shap.Explainer(our_rf_model)\n",
    "                shap_values = explainer(X_test)\n",
    "                # shap.plots.bar(shap_values[file_to_be_explained_idx, :, 1], show=True)\n",
    "                explain_file_val = shap_values[file_to_be_explained_idx, :, 1].values.tolist()\n",
    "                shap_dict = {}\n",
    "                test_columns = X_test.columns.values.tolist()\n",
    "                for col_index in range(0, len(test_columns)):\n",
    "                    shap_dict[test_columns[col_index]] = explain_file_val[col_index]\n",
    "                sorted_shap_dict = dict(sorted(shap_dict.items(), key=lambda item: abs(item[1]), reverse=True))\n",
    "                for keys in sorted_shap_dict.keys():\n",
    "                    if sorted_shap_dict[keys] >= 0:\n",
    "                        sorted_shap_dict[keys] = 1\n",
    "                    else:\n",
    "                        sorted_shap_dict[keys] = -1\n",
    "                print(sorted_shap_dict)\n",
    "                \n",
    "                \n",
    "                #Breakdown\n",
    "                our_rf_model = RandomForestClassifier(random_state=0)\n",
    "                our_rf_model.fit(X_train, y_train)\n",
    "                #make explainer object\n",
    "                exp = Explainer(clf=our_rf_model, data= X_train, colnames=features)\n",
    "                #make explanation object that contains all information\n",
    "                explanation = exp.explain(observation=X_test.iloc[file_to_be_explained_idx,:].values,direction=\"up\",useIntercept=True)\n",
    "                # explanation.visualize()\n",
    "                output_buffer = io.StringIO()\n",
    "                with contextlib.redirect_stdout(output_buffer):\n",
    "                    explanation.text()\n",
    "\n",
    "                captured_output = output_buffer.getvalue()\n",
    "                with open(\"test.txt\", \"w\") as file:\n",
    "                    file.write(captured_output)\n",
    "                    \n",
    "                breakdown_df = pd.read_csv(\"test.txt\", sep='\\t')\n",
    "                \n",
    "                counter=-1\n",
    "                breakdown_dict={}\n",
    "                for _, row in breakdown_df.iterrows():\n",
    "                    counter+=1\n",
    "                    if counter < 1:\n",
    "                        continue\n",
    "                    row_val = row.values.tolist()\n",
    "                    row_val = row_val[0].split(\" \")\n",
    "                    row_val_main = []\n",
    "                    for c in row_val:\n",
    "                        if len(c) == 0 or c == \"=\":\n",
    "                            continue\n",
    "                        row_val_main.append(c)\n",
    "                    if float(row_val_main[2]) >= 0:\n",
    "                        breakdown_dict[row_val_main[0]] = 1\n",
    "                    else:\n",
    "                        breakdown_dict[row_val_main[0]] = -1\n",
    "                    if counter == len(features):\n",
    "                        break\n",
    "                print(breakdown_dict)\n",
    "                \n",
    "                \n",
    "                #### pyexplainer\n",
    "                rf_model = RandomForestClassifier(random_state=0)\n",
    "                rf_model.fit(X_train, y_train) \n",
    "                np.random.seed(0)\n",
    "                pyexp = PyExplainer(X_train = X_train,\n",
    "                                           y_train = y_train,\n",
    "                                           indep = X_train.columns,\n",
    "                                           dep = outcome,\n",
    "                                           top_k_rules=5,\n",
    "                                           blackbox_model = rf_model)\n",
    "\n",
    "                # PyExplainer Step 2 - Generate the rule-based explanation of an instance to be explained\n",
    "                exp_obj = pyexp.explain(X_explain = X_test.loc[file_to_be_explained,:].to_frame().transpose(),\n",
    "                                        y_explain = pd.Series(bool(y_test.loc[file_to_be_explained]), \n",
    "                                                                      index = [file_to_be_explained],\n",
    "                                                                      name = outcome),\n",
    "                                        search_function = 'crossoverinterpolation',\n",
    "                                        max_iter=1000,\n",
    "                                        max_rules=20,\n",
    "                                        random_state=0,\n",
    "                                        reuse_local_model=True)\n",
    "\n",
    "                # Please use the code below to visualise the generated PyExplainer explanation (What-If interactive visualisation).\n",
    "                pyexp.visualise(exp_obj, title=\"Why this file is predicted as defect-introducing?\")\n",
    "                top_rules = pyexp.parse_top_rules(exp_obj['top_k_positive_rules'],exp_obj['top_k_negative_rules'])['top_tofollow_rules']\n",
    "                bullet_data = pyexp.generate_bullet_data(pyexp.parse_top_rules(exp_obj['top_k_positive_rules'],exp_obj['top_k_negative_rules']))\n",
    "                pyexp_dict = {}\n",
    "                for rules in range(0, len(top_rules)):\n",
    "                    variable = top_rules[rules]['variable']\n",
    "                    value = float(top_rules[rules]['value'])\n",
    "                    marker = bullet_data[rules]['markers'][0]\n",
    "                    if top_rules[rules]['lessthan']:\n",
    "                        if marker <= value:\n",
    "                            pyexp_dict[variable] = 1\n",
    "                        else:\n",
    "                            pyexp_dict[variable] = -1\n",
    "                    else:\n",
    "                        if marker > value:\n",
    "                            pyexp_dict[variable] = 1\n",
    "                        else:\n",
    "                            pyexp_dict[variable] = -1\n",
    "\n",
    "                print(pyexp_dict)\n",
    "                worked=False\n",
    "                tested_file.append(file_to_be_explained)\n",
    "                    \n",
    "                data_task1 = np.array([\n",
    "                    [calculate_task1(lime_dict, lime_dict), calculate_task1(lime_dict, sorted_shap_dict), calculate_task1(lime_dict, breakdown_dict), calculate_task1(lime_dict, pyexp_dict)],\n",
    "                    [calculate_task1(sorted_shap_dict, lime_dict), calculate_task1(sorted_shap_dict, sorted_shap_dict), calculate_task1(sorted_shap_dict, breakdown_dict), calculate_task1(sorted_shap_dict, pyexp_dict)],\n",
    "                    [calculate_task1(breakdown_dict, lime_dict), calculate_task1(breakdown_dict, sorted_shap_dict), calculate_task1(breakdown_dict, breakdown_dict), calculate_task1(breakdown_dict, pyexp_dict)],\n",
    "                    [calculate_task1(pyexp_dict, lime_dict), calculate_task1(pyexp_dict, sorted_shap_dict), calculate_task1(pyexp_dict, breakdown_dict), calculate_task1(pyexp_dict, pyexp_dict)]\n",
    "                ])\n",
    "\n",
    "                data_task2 = np.array([\n",
    "                    [calculate_task2(lime_dict, lime_dict), calculate_task2(lime_dict, sorted_shap_dict), calculate_task2(lime_dict, breakdown_dict), calculate_task2(lime_dict, pyexp_dict)],\n",
    "                    [calculate_task2(sorted_shap_dict, lime_dict), calculate_task2(sorted_shap_dict, sorted_shap_dict), calculate_task2(sorted_shap_dict, breakdown_dict), calculate_task2(sorted_shap_dict, pyexp_dict)],\n",
    "                    [calculate_task2(breakdown_dict, lime_dict), calculate_task2(breakdown_dict, sorted_shap_dict), calculate_task2(breakdown_dict, breakdown_dict), calculate_task2(breakdown_dict, pyexp_dict)],\n",
    "                    [calculate_task2(pyexp_dict, lime_dict), calculate_task2(pyexp_dict, sorted_shap_dict), calculate_task2(pyexp_dict, breakdown_dict), calculate_task2(pyexp_dict, pyexp_dict)]\n",
    "                ])\n",
    "\n",
    "                data_task3 = np.array([\n",
    "                    [calculate_task3(lime_dict, lime_dict), calculate_task3(lime_dict, sorted_shap_dict), calculate_task3(lime_dict, breakdown_dict), calculate_task3(lime_dict, pyexp_dict)],\n",
    "                    [calculate_task3(sorted_shap_dict, lime_dict), calculate_task3(sorted_shap_dict, sorted_shap_dict), calculate_task3(sorted_shap_dict, breakdown_dict), calculate_task3(sorted_shap_dict, pyexp_dict)],\n",
    "                    [calculate_task3(breakdown_dict, lime_dict), calculate_task3(breakdown_dict, sorted_shap_dict), calculate_task3(breakdown_dict, breakdown_dict), calculate_task3(breakdown_dict, pyexp_dict)],\n",
    "                    [calculate_task3(pyexp_dict, lime_dict), calculate_task3(pyexp_dict, sorted_shap_dict), calculate_task3(pyexp_dict, breakdown_dict), calculate_task3(pyexp_dict, pyexp_dict)]\n",
    "                ])\n",
    "\n",
    "                # Row and column labels\n",
    "                pairs = [\"LIME\", \"SHAP\", \"BreakDown\", \"PyExplainer\"]\n",
    "                tasks = [\"Feature Agreement\", \"Rank Agreement\", \"Sign Agreement\"]\n",
    "\n",
    "                # Create the figure and subplots\n",
    "                fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "                fig.suptitle(\"Processing file: \"+main_file.split(\"/\")[-1])\n",
    "\n",
    "                # Plot heatmap for FA\n",
    "                sns.heatmap(data_task1, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=.5, xticklabels=pairs, yticklabels=pairs, ax=axes[0])\n",
    "                axes[0].set_title(tasks[0] + \" (When k=\"+str(len(features))+\")\")\n",
    "                \n",
    "                # Plot heatmap for SA\n",
    "                sns.heatmap(data_task3, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=.5, xticklabels=pairs, yticklabels=pairs, ax=axes[1])\n",
    "                axes[1].set_title(tasks[2] + \" (When k=\"+str(len(features))+\")\")\n",
    "\n",
    "                # Plot heatmap for RA\n",
    "                sns.heatmap(data_task2, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=.5, xticklabels=pairs, yticklabels=pairs, ax=axes[2])\n",
    "                axes[2].set_title(tasks[1] + \" (When k=\"+str(len(features))+\")\")\n",
    "\n",
    "                plt.tight_layout()\n",
    "                fig_file_name = main_file.split(\"/\")[-1].replace(\".\", \"_\")+\"-\"+file_to_be_explained.replace(\"/\", \"_\").replace(\".\",\"_\")+\"_fa_ra_sa_plot.png\"\n",
    "                plt.savefig(fig_file_name)\n",
    "                \n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "        except Exception as e:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7c1354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b04fe15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e209ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cac78b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828fbee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a4e3a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa531186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e6cacd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
